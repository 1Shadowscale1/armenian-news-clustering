import pandas as pd
import numpy as np
from typing import List, Dict, Any
from datetime import datetime
import json


class ClusterAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""

    def __init__(self):
        pass

    def analyze_combined_clusters(self, df: pd.DataFrame, cluster_labels: np.ndarray,
                                  input_texts: List[str], dates: List,
                                  ner_model: Any) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π"""
        clusters = {}

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        print("üîç Precomputing named entities for all texts...")
        all_entities_batch = ner_model.get_named_entities_batch(input_texts)

        for i, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = {
                    'items': [],
                    'common_entities': set(),
                    'date_range': None,
                    'entities_by_type': {'PER': set(), 'LOC': set(), 'ORG': set(), 'MISC': set()}
                }

            entities_set = all_entities_batch[i] if i < len(all_entities_batch) else set()
            clusters[label]['common_entities'].update(entities_set)

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º
            for entity in entities_set:
                if ':' in entity:
                    entity_type, entity_text = entity.split(':', 1)
                    if entity_type in clusters[label]['entities_by_type']:
                        clusters[label]['entities_by_type'][entity_type].add(entity)
                else:
                    clusters[label]['entities_by_type']['MISC'].add(f"MISC:{entity}")

            clusters[label]['items'].append({
                'text': input_texts[i],
                'title': df.iloc[i]['title'],
                'date': dates[i],
                'original_index': i,
                'entities': entities_set
            })

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        for label, cluster_data in clusters.items():
            if cluster_data['items']:
                cluster_dates = [item['date'] for item in cluster_data['items']]
                cluster_data['date_range'] = {
                    'min': min(cluster_dates),
                    'max': max(cluster_dates),
                    'span_days': (max(cluster_dates) - min(cluster_dates)).days
                }

        return clusters

    def analyze_cluster_quality(self, clusters: Dict, similarity_matrix: np.ndarray) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        quality_metrics = {
            'total_clusters': len([k for k in clusters.keys() if k != -1]),
            'total_articles': sum(len(c['items']) for c in clusters.values()),
            'noise_articles': len(clusters.get(-1, {}).get('items', [])),
            'cluster_sizes': {},
            'avg_intra_similarity': {}
        }

        for cluster_id, cluster_info in clusters.items():
            if cluster_id == -1:
                continue

            cluster_indices = [item['original_index'] for item in cluster_info['items']]
            quality_metrics['cluster_sizes'][cluster_id] = len(cluster_indices)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            intra_similarity = 0
            pair_count = 0
            for i in range(len(cluster_indices)):
                for j in range(i + 1, len(cluster_indices)):
                    intra_similarity += similarity_matrix[cluster_indices[i], cluster_indices[j]]
                    pair_count += 1

            avg_similarity = intra_similarity / pair_count if pair_count > 0 else 0
            quality_metrics['avg_intra_similarity'][cluster_id] = avg_similarity

        return quality_metrics

    def create_cluster_news_mapping(self, clusters: Dict, output_file: str = None) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"""
        cluster_news_mapping = {}

        for cluster_id, cluster_info in clusters.items():
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (-1)
            if int(cluster_id) == -1:
                continue

            news_ids = [item['original_index'] for item in cluster_info['items']]
            cluster_news_mapping[int(cluster_id)] = news_ids

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        if -1 in clusters:
            noise_articles = [item['original_index'] for item in clusters[-1]['items']]
            if noise_articles:
                cluster_news_mapping[-1] = noise_articles

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cluster_news_mapping, f, indent=2, ensure_ascii=False)
            print(f"Cluster-news mapping saved to: {output_file}")

        return cluster_news_mapping