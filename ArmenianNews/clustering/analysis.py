import pandas as pd
import numpy as np
from typing import List, Dict, Any
from datetime import datetime
import json


class ClusterAnalyzer:
    """ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹"""

    def __init__(self):
        pass

    def analyze_combined_clusters(self, df: pd.DataFrame, cluster_labels: np.ndarray,
                                  input_texts: List[str], dates: List,
                                  ner_model: Any) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹"""
        clusters = {}

        # ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹
        print("ğŸ” Precomputing named entities for all texts...")
        all_entities_batch = ner_model.get_named_entities_batch(input_texts)

        for i, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = {
                    'items': [],
                    'common_entities': set(),
                    'date_range': None,
                    'entities_by_type': {'PER': set(), 'LOC': set(), 'ORG': set(), 'MISC': set()}
                }

            entities_set = all_entities_batch[i] if i < len(all_entities_batch) else set()
            clusters[label]['common_entities'].update(entities_set)

            # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼
            for entity in entities_set:
                if ':' in entity:
                    entity_type, entity_text = entity.split(':', 1)
                    if entity_type in clusters[label]['entities_by_type']:
                        clusters[label]['entities_by_type'][entity_type].add(entity)
                else:
                    clusters[label]['entities_by_type']['MISC'].add(f"MISC:{entity}")

            clusters[label]['items'].append({
                'text': input_texts[i],
                'title': df.iloc[i]['title'],
                'date': dates[i],
                'original_index': i,
                'entities': entities_set
            })

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        for label, cluster_data in clusters.items():
            if cluster_data['items']:
                cluster_dates = [item['date'] for item in cluster_data['items']]
                cluster_data['date_range'] = {
                    'min': min(cluster_dates),
                    'max': max(cluster_dates),
                    'span_days': (max(cluster_dates) - min(cluster_dates)).days
                }

        return clusters

    def analyze_cluster_quality(self, clusters: Dict, similarity_matrix: np.ndarray) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ²"""
        quality_metrics = {
            'total_clusters': len([k for k in clusters.keys() if k != -1]),
            'total_articles': sum(len(c['items']) for c in clusters.values()),
            'noise_articles': len(clusters.get(-1, {}).get('items', [])),
            'cluster_sizes': {},
            'avg_intra_similarity': {}
        }

        for cluster_id, cluster_info in clusters.items():
            if cluster_id == -1:
                continue

            cluster_indices = [item['original_index'] for item in cluster_info['items']]
            quality_metrics['cluster_sizes'][cluster_id] = len(cluster_indices)

            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸
            intra_similarity = 0
            pair_count = 0
            for i in range(len(cluster_indices)):
                for j in range(i + 1, len(cluster_indices)):
                    intra_similarity += similarity_matrix[cluster_indices[i], cluster_indices[j]]
                    pair_count += 1

            avg_similarity = intra_similarity / pair_count if pair_count > 0 else 0
            quality_metrics['avg_intra_similarity'][cluster_id] = avg_similarity

        return quality_metrics

    def create_cluster_news_mapping(self, clusters: Dict, output_file: str = None) -> Dict:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸"""
        cluster_news_mapping = {}

        for cluster_id, cluster_info in clusters.items():
            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹ (-1)
            if int(cluster_id) == -1:
                continue

            news_ids = [item['original_index'] for item in cluster_info['items']]
            cluster_news_mapping[int(cluster_id)] = news_ids

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
        if -1 in clusters:
            noise_articles = [item['original_index'] for item in clusters[-1]['items']]
            if noise_articles:
                cluster_news_mapping[-1] = noise_articles

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ„Ğ°Ğ¹Ğ», ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ¿ÑƒÑ‚ÑŒ
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cluster_news_mapping, f, indent=2, ensure_ascii=False)
            print(f"Cluster-news mapping saved to: {output_file}")

        return cluster_news_mapping

    def get_cluster_summary(self, clusters: Dict) -> str:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ´ĞºĞ¸ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ğ¼"""
        summary_lines = []
        summary_lines.append("=" * 80)
        summary_lines.append("CLUSTER ANALYSIS SUMMARY:")
        summary_lines.append("=" * 80)

        valid_clusters = [k for k in clusters.keys() if k != -1]
        noise_count = len(clusters.get(-1, {}).get('items', []))
        total_in_clusters = sum(len(clusters[k]['items']) for k in valid_clusters)

        summary_lines.append(f"\nğŸ“Š Total clusters: {len(valid_clusters)}")
        summary_lines.append(f"ğŸ“ Articles in clusters: {total_in_clusters}")
        summary_lines.append(f"ğŸ­ Noise articles: {noise_count}")
        summary_lines.append(f"ğŸ“ˆ Total articles: {total_in_clusters + noise_count}")

        summary_lines.append("\nğŸ” Cluster Details:")
        for cluster_id in sorted(valid_clusters):
            cluster_info = clusters[cluster_id]
            summary_lines.append(f"\n  Cluster {cluster_id}:")
            summary_lines.append(f"    ğŸ“ Size: {len(cluster_info['items'])} articles")

            # ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸
            key_entities = []
            for entity_type in ['PER', 'LOC', 'ORG']:
                entities = list(cluster_info['entities_by_type'][entity_type])
                if entities:
                    key_entities.extend(entities[:2])

            if key_entities:
                summary_lines.append(f"    ğŸ”‘ Key entities: {', '.join(key_entities[:5])}")

            # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°
            if cluster_info['items']:
                first_item = cluster_info['items'][0]
                summary_lines.append(f"    ğŸ“° Sample: '{first_item['title'][:70]}...'")

        return "\n".join(summary_lines)