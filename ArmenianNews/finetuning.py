import gc
import os
import warnings
from typing import List, Dict

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

from ArmenianNews import (
    ArmenianNewsDataLoader,
    ArmenianTextPreprocessor
)
from .models.dataset import OptimizedTripletDataset
from .models.embedding_model import ArmenianEmbeddingModel, TripletLoss

warnings.filterwarnings('ignore')


class FineTuningManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è fine-tuning –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ –∞—Ä–º—è–Ω—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç—è—Ö"""

    def __init__(self, model_name: str = "Metric-AI/armenian-text-embeddings-1",
                 device: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ fine-tuning

        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ('cuda' –∏–ª–∏ 'cpu')
        """
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_name = model_name

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        print(f"Initializing fine-tuning on {self.device}...")
        self.model = ArmenianEmbeddingModel(model_name, device)
        self.tokenizer = self.model.tokenizer

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        torch.backends.cudnn.benchmark = True
        if self.device == 'cuda':
            torch.backends.cuda.matmul.allow_tf32 = True

    def prepare_training_data(self, file_paths: List[str], sample_size: int = 100) -> Dict:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ CSV —Ñ–∞–π–ª–∞–º
            sample_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        print("Loading data...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data_loader = ArmenianNewsDataLoader.load_data_optimized(file_paths, sample_size)
        df = data_loader

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç
        print("Converting dates...")
        df['date_time'] = df['date_time'].apply(ArmenianTextPreprocessor.convert_armenian_date)
        df = df.dropna(subset=['date_time']).reset_index(drop=True)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        print("Preparing texts...")
        input_texts = []
        for idx, row in df.iterrows():
            text = f"{row['title']}. {row['text']}"
            # –û–±—Ä–µ–∑–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            if len(text) > 1000:
                text = text[:1000]
            input_texts.append(text)

        print(f"‚úÖ Processing {len(input_texts)} articles...")

        return {
            'dataframe': df,
            'texts': input_texts,
            'dates': df['date_time'].tolist()
        }

    def create_triplets(self, df, input_texts: List[str], dates: List,
                        n_triplets: int = 400) -> List[Dict]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            input_texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            dates: –°–ø–∏—Å–æ–∫ –¥–∞—Ç
            n_triplets: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤
        """
        print("Creating triplets...")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤
        max_triplets = min(n_triplets, len(df) // 3)
        if max_triplets < 10:
            warnings.warn(f"Not enough data for triplets: only {len(df)} articles")
            max_triplets = max(10, len(df) // 2)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ triplet_generation
        from .triplet_generation import TripletGenerator
        generator = TripletGenerator(n_triplets=max_triplets)

        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∏–ø–ª–µ—Ç—ã —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
        semantic_triplets = generator.create_semantic_triplets_tfidf(input_texts)
        length_triplets = generator.create_length_based_triplets(input_texts)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —Ç—Ä–∏–ø–ª–µ—Ç—ã
        all_triplets = semantic_triplets + length_triplets

        print(f"Created {len(all_triplets)} triplets")

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        semantic_count = len([t for t in all_triplets if t.get('type') == 'semantic_tfidf'])
        length_count = len([t for t in all_triplets if t.get('type') == 'length_based'])

        print(f"   - Semantic triplets: {semantic_count}")
        print(f"   - Length-based triplets: {length_count}")

        return all_triplets

    def train_epoch(self, dataloader: DataLoader, optimizer: torch.optim.Optimizer,
                    accumulation_steps: int = 4) -> float:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ

        Args:
            dataloader: DataLoader —Å –¥–∞–Ω–Ω—ã–º–∏
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            accumulation_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

        Returns:
            –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ loss –Ω–∞ —ç–ø–æ—Ö–µ
        """
        self.model.model.train()
        total_loss = 0
        loss_fn = TripletLoss(margin=1.0)

        optimizer.zero_grad()

        with tqdm(dataloader, desc="Training", unit="batch") as pbar:
            for batch_idx, batch in enumerate(pbar):
                # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                anchor_inputs = {k: v.to(self.device) for k, v in batch['anchor'].items()}
                positive_inputs = {k: v.to(self.device) for k, v in batch['positive'].items()}
                negative_inputs = {k: v.to(self.device) for k, v in batch['negative'].items()}

                # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                with torch.cuda.amp.autocast(enabled=self.device == 'cuda'):
                    anchor_outputs = self.model.model(**anchor_inputs)
                    positive_outputs = self.model.model(**positive_inputs)
                    negative_outputs = self.model.model(**negative_inputs)

                    # Pooling —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    anchor_embeddings = self.model.average_pool(
                        anchor_outputs.last_hidden_state,
                        anchor_inputs['attention_mask']
                    )
                    positive_embeddings = self.model.average_pool(
                        positive_outputs.last_hidden_state,
                        positive_inputs['attention_mask']
                    )
                    negative_embeddings = self.model.average_pool(
                        negative_outputs.last_hidden_state,
                        negative_inputs['attention_mask']
                    )

                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    anchor_embeddings = F.normalize(anchor_embeddings, p=2, dim=1)
                    positive_embeddings = F.normalize(positive_embeddings, p=2, dim=1)
                    negative_embeddings = F.normalize(negative_embeddings, p=2, dim=1)

                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
                    loss = loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)
                    loss = loss / accumulation_steps

                # Backward pass
                loss.backward()

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø–æ—Å–ª–µ accumulation_steps
                if (batch_idx + 1) % accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()

                total_loss += loss.item() * accumulation_steps
                pbar.set_postfix({'loss': loss.item() * accumulation_steps})

        return total_loss / len(dataloader)

    def fine_tune(self, file_paths: List[str], output_dir: str,
                  num_epochs: int = 3, batch_size: int = 4,
                  learning_rate: float = 1e-5, n_triplets: int = 400) -> Dict:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è fine-tuning

        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –¥–∞–Ω–Ω—ã–º
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            n_triplets: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        print("Starting fine-tuning process...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data = self.prepare_training_data(file_paths)
        df = data['dataframe']
        input_texts = data['texts']
        dates = data['dates']

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤
        triplets = self.create_triplets(df, input_texts, dates, n_triplets)

        if len(triplets) < 10:
            raise ValueError(f"Not enough triplets created: {len(triplets)}. Need at least 10.")

        # –°–æ–∑–¥–∞–Ω–∏–µ dataset –∏ dataloader
        print("Creating dataset and dataloader...")
        dataset = OptimizedTripletDataset(
            texts=input_texts,
            triplets=triplets,
            tokenizer=self.tokenizer,
            max_length=128
        )

        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=True if self.device == 'cuda' else False
        )

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = torch.optim.AdamW(
            self.model.model.parameters(),
            lr=learning_rate,
            weight_decay=0.01
        )

        # –û–±—É—á–µ–Ω–∏–µ
        print(f"Training for {num_epochs} epochs...")
        training_history = {
            'epochs': [],
            'losses': [],
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'triplets_used': len(triplets)
        }

        for epoch in range(num_epochs):
            print(f"\nüìà Epoch {epoch + 1}/{num_epochs}")

            avg_loss = self.train_epoch(dataloader, optimizer, accumulation_steps=4)
            training_history['epochs'].append(epoch + 1)
            training_history['losses'].append(avg_loss)

            print(f"üìâ Average loss: {avg_loss:.4f}")

            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if self.device == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print(f"Saving model to {output_dir}...")
        os.makedirs(output_dir, exist_ok=True)

        self.model.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        print("Fine-tuning completed successfully!")

        return training_history

    def evaluate_fine_tuning(self, test_file_paths: List[str],
                             original_model_name: str = None) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ fine-tuning

        Args:
            test_file_paths: –ü—É—Ç–∏ –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
            original_model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
        """
        print("Evaluating fine-tuning results...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_data = self.prepare_training_data(test_file_paths, sample_size=50)
        test_texts = test_data['texts']

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ –∏ –ø–æ—Å–ª–µ fine-tuning
        print("Getting embeddings from fine-tuned model...")
        ft_embeddings = self.model.get_embeddings_batch(test_texts, batch_size=8)

        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –∏—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –Ω–µ–π
        comparison_results = {}
        if original_model_name:
            print(f"Comparing with original model: {original_model_name}")
            original_model = ArmenianEmbeddingModel(original_model_name, self.device)
            original_embeddings = original_model.get_embeddings_batch(test_texts, batch_size=8)

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
            from sklearn.metrics.pairwise import cosine_similarity
            similarity_matrix = cosine_similarity(ft_embeddings.numpy(), original_embeddings.numpy())
            avg_similarity = similarity_matrix.diagonal().mean()

            comparison_results = {
                'avg_similarity_to_original': avg_similarity,
                'similarity_matrix': similarity_matrix
            }

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        n_clusters = min(5, len(test_texts) // 3)
        if n_clusters >= 2:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(ft_embeddings.numpy())
            silhouette = silhouette_score(ft_embeddings.numpy(), cluster_labels)

            comparison_results['clustering_quality'] = {
                'silhouette_score': silhouette,
                'n_clusters': n_clusters,
                'cluster_distribution': np.bincount(cluster_labels)
            }

        print("Evaluation completed!")
        return comparison_results


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ fine-tuning
def fine_tune_armenian_model(file_paths: List[str], output_dir: str = "./fine_tuned_model",
                             model_name: str = "Metric-AI/armenian-text-embeddings-1",
                             num_epochs: int = 3, batch_size: int = 4,
                             learning_rate: float = 1e-5, n_triplets: int = 400) -> Dict:
    """
    –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ fine-tuning –º–æ–¥–µ–ª–∏ –Ω–∞ –∞—Ä–º—è–Ω—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç—è—Ö

    Args:
        file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ CSV —Ñ–∞–π–ª–∞–º —Å –¥–∞–Ω–Ω—ã–º–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        n_triplets: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    ft_manager = FineTuningManager(model_name=model_name)

    # –ó–∞–ø—É—Å–∫ fine-tuning
    results = ft_manager.fine_tune(
        file_paths=file_paths,
        output_dir=output_dir,
        num_epochs=num_epochs,
        batch_size=batch_size,
        learning_rate=learning_rate,
        n_triplets=n_triplets
    )

    return results


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
def main():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å fine-tuning
    """
    import argparse

    parser = argparse.ArgumentParser(description='Fine-tune Armenian text embedding model')
    parser.add_argument('--data_paths', nargs='+', required=True,
                        help='Paths to CSV files with Armenian news')
    parser.add_argument('--output_dir', default='./fine_tuned_model',
                        help='Directory to save fine-tuned model')
    parser.add_argument('--model_name', default='Metric-AI/armenian-text-embeddings-1',
                        help='Name of the pretrained model')
    parser.add_argument('--epochs', type=int, default=3,
                        help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=4,
                        help='Training batch size')
    parser.add_argument('--learning_rate', type=float, default=1e-5,
                        help='Learning rate')
    parser.add_argument('--n_triplets', type=int, default=400,
                        help='Number of triplets to create')

    args = parser.parse_args()

    # –ó–∞–ø—É—Å–∫ fine-tuning
    print("üöÄ Starting Armenian News Model Fine-Tuning")
    print("=" * 50)
    print(f"Model: {args.model_name}")
    print(f"Data: {len(args.data_paths)} files")
    print(f"Output: {args.output_dir}")
    print(f"Epochs: {args.epochs}, Batch size: {args.batch_size}")
    print("=" * 50)

    try:
        results = fine_tune_armenian_model(
            file_paths=args.data_paths,
            output_dir=args.output_dir,
            model_name=args.model_name,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            n_triplets=args.n_triplets
        )

        print("\n" + "=" * 50)
        print("FINE-TUNING COMPLETED SUCCESSFULLY")
        print("=" * 50)
        print(f"Model saved to: {args.output_dir}")
        print(f"Training history: {results}")

    except Exception as e:
        print(f"Error during fine-tuning: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()