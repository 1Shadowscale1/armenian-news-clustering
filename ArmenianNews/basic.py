"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–∫–µ—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∞—Ä–º—è–Ω—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from ArmenianNews import (
    ArmenianNewsDataLoader,
    ArmenianTextPreprocessor,
    ArmenianEmbeddingModel,
    ArmenianNERModel
)
from ArmenianNews.clustering.similarity import SimilarityCalculator
from ArmenianNews.clustering.clustering import NewsClustering
from ArmenianNews.clustering.analysis import ClusterAnalyzer
from ArmenianNews.utils.visualization import NewsVisualization


def pipeline():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    NUMBER_OF_ARTICLES_FROM_ONE_SOURCE = 100
    file_paths = [
        '/kaggle/input/armenian-political-news/armeniatoday.csv',
        '/kaggle/input/armenian-political-news/armenpress.csv',
        '/kaggle/input/armenian-political-news/hetq.csv',
        '/kaggle/input/armenian-political-news/sputnik.csv',
        '/kaggle/input/armenian-political-news/tert.csv'
    ]

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("Loading data...")
    data_loader = ArmenianNewsDataLoader(sample_size=NUMBER_OF_ARTICLES_FROM_ONE_SOURCE)
    df = data_loader.load_data_optimized(file_paths)

    # 2. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
    print("Preprocessing data...")
    preprocessor = ArmenianTextPreprocessor()
    df = preprocessor.preprocess_dataframe(df, date_column='date_time')

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
    input_texts = df['full_text'].tolist()
    n_articles = len(input_texts)
    print(f"Processing {n_articles} articles...")

    # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print("Computing embeddings...")
    embedding_model = ArmenianEmbeddingModel()
    embeddings = embedding_model.get_embeddings_batch(input_texts)
    dates = df['date_time'].tolist()

    print("Embeddings computed successfully!")

    # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    print("Extracting named entities...")
    ner_model = ArmenianNERModel()
    all_entities = ner_model.get_named_entities_batch(input_texts)

    print("Named entities extracted successfully")

    # 5. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏
    print("Computing similarity matrix...")
    similarity_calculator = SimilarityCalculator()
    similarity_matrix, semantic_matrix, entity_matrix, time_matrix, length_matrix = similarity_calculator.compute_similarity_matrix(
        embeddings=embeddings,
        all_entities=all_entities,
        dates=dates,
        texts=input_texts
    )

    print("Similarity matrix computed successfully")

    # 6. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    print("\n=== Improved Clustering based on Combined Similarity ===\n")
    clustering = NewsClustering(min_cluster_size=3)
    cluster_labels = clustering.optimized_clustering(similarity_matrix)

    print(f"Initial clustering completed. Found {len(np.unique(cluster_labels))} clusters")
    print(f"Cluster distribution: {np.bincount(cluster_labels + 1)}")

    # 7. –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print("\nAnalyzing clusters...")
    analyzer = ClusterAnalyzer()
    clusters = analyzer.analyze_combined_clusters(
        df=df,
        cluster_labels=cluster_labels,
        input_texts=input_texts,
        dates=dates,
        ner_model=ner_model
    )

    # 8. –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print("Post-processing clusters...")
    clusters = clustering.post_process_clusters(clusters, similarity_matrix)

    # 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print("\nCreating visualization...")
    visualizer = NewsVisualization()

    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    updated_cluster_labels = np.full(len(df), -1)
    for cluster_id, cluster_info in clusters.items():
        if cluster_id != -1:
            for item in cluster_info['items']:
                updated_cluster_labels[item['original_index']] = cluster_id

    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    fig = visualizer.create_cluster_visualization(
        embeddings=embeddings.numpy(),
        cluster_labels=updated_cluster_labels,
        df=df
    )
    plt.show()

    # 12. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    noise_count = np.sum(updated_cluster_labels == -1)
    valid_clusters = len([c for c in clusters.keys() if c != -1 and len(clusters[c]['items']) >= 2])

    print(f"\nüìä IMPROVED CLUSTERING SUMMARY:")
    print(f"   Total articles: {len(df)}")
    print(f"   Valid clusters: {valid_clusters}")
    print(f"   Noise articles: {noise_count}")
    print(f"   Articles in clusters: {len(df) - noise_count}")


    # 13. –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
    print("\nüìä Creating cluster-news mapping...")
    cluster_news_mapping = analyzer.create_cluster_news_mapping(
        clusters=clusters,
        output_file='/kaggle/working/cluster_news_mapping.json'
    )